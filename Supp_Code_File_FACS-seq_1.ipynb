{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA folding kinetics control riboswitch sensitivity in vivo \n",
    "#### David Z. Bushhouse1,3 , Jiayu Fu1,3, & Julius B. Lucks1,2,3,4,5,6* \n",
    "\n",
    " \n",
    "\n",
    "1 Interdisciplinary Biological Sciences Graduate Program, Northwestern University, Evanston, Illinois 60208, USA \n",
    "\n",
    "2 Department of Chemical and Biological Engineering, Northwestern University, Evanston, Illinois 60208, USA \n",
    "\n",
    "3 Center for Synthetic Biology, Northwestern University, Evanston, Illinois 60208, USA \n",
    "\n",
    "4 Center for Water Research, Northwestern University, Evanston, Illinois 60208, USA \n",
    "\n",
    "5 Center for Engineering Sustainability and Resilience, Northwestern University, Evanston, Illinois 60208, USA \n",
    "\n",
    "6 International Institute for Nanotechnology, Northwestern University, Evanston, Illinois 60208, USA \n",
    "\n",
    "\n",
    "* To whom correspondence should be addressed. Tel: 1-847-467-2943; Email: jblucks@northwestern.edu  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing sequencing data and estimating mean fluorescence for each loop sequence at each ligand condition\n",
    "This code is part 1 of the FACS-seq analysis pipeline. Ensure that sequencing reads (fastq) from SRA Deposition PRJNA1087340 and cell count metadata files are in the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary packages and set up file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import shutil\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify were the raw data is read and where the analysis files are stored.\n",
    "# Define the directory where your command (script) files reside\n",
    "command_path = 'PATH-TO-SRA-DEPOSITION'\n",
    "# Define the directory where your data files are located\n",
    "data_path = 'PATH-TO-SRA-DEPOSITION'\n",
    "# Define the analysis subfolder and create it if it doesn't exist\n",
    "analysis_path = os.path.join(data_path, 'Analysis')\n",
    "if not os.path.exists(analysis_path):\n",
    "    os.makedirs(analysis_path)\n",
    "\n",
    "# Define the experimental prefix\n",
    "experimental_prefix = 'Rep1'\n",
    "experimental_reads = experimental_prefix + \"*.fastq\"\n",
    "\n",
    "# Ensure that the Cell Count files are in the same directory\n",
    "cell_counts = experimental_prefix + '_CellCounts.txt'\n",
    "\n",
    "# Define the loop length range\n",
    "min_loop_length = 7\n",
    "max_loop_length = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality threshold and removal of non-matching PE reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the markers for R1\n",
    "#start_marker_r1 is the sequence in the ZTP riboswitch upstream of the 7nt Loop sequence from Read 1 (top strand 5'-3')\n",
    "start_marker_r1 = \"AAAAAGCCGACCGTCTGGGC\"\n",
    "#end_marker_r1 is the sequence in the ZTP riboswitch downstream of the 7nt Loop sequence from Read 1(top strand 5'-3')\n",
    "end_marker_r1 = \"CCTGGATTGCGTCGGCTTTT\"\n",
    "# Define the markers for R2\n",
    "#start_marker_r2 is the sequence in the ZTP riboswitch upstream of the 7nt Loop sequence from Read 2 (bottom strand 5'-3')\n",
    "start_marker_r2 = \"AAAAGCCGACGCAATCCAGG\"\n",
    "#end_marker_r2 is the sequence in the ZTP riboswitch downstream of the 7nt Loop sequence from Read 2(top strand 5'-3')\n",
    "end_marker_r2 = \"GCCCAGACGGTCGGCTTTTT\"\n",
    "## These should be reverse compliment of each other\n",
    "\n",
    "# Function to generate reverse complement of Loop sequence for match processes\n",
    "def reverse_complement(seq):\n",
    "    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C', 'N': 'N'}\n",
    "    return ''.join(complement[base] for base in reversed(seq))\n",
    "\n",
    "# Function to filter out reads with low quality score\n",
    "def filter_reads_by_qscore(read1_file, read2_file, output_read1, output_read2, threshold=30):\n",
    "    with open(read1_file, 'r') as r1, open(read2_file, 'r') as r2, \\\n",
    "         open(output_read1, 'w') as o1, open(output_read2, 'w') as o2:\n",
    "         \n",
    "         while True:\n",
    "            r1_lines = [r1.readline().strip() for _ in range(4)]\n",
    "            r2_lines = [r2.readline().strip() for _ in range(4)]\n",
    "            \n",
    "            if not r1_lines[0] or not r2_lines[0]:  # End of file\n",
    "                break\n",
    "\n",
    "            r1_qual = r1_lines[3]\n",
    "            r2_qual = r2_lines[3]\n",
    "            \n",
    "            if all(ord(ch) - 33 >= threshold for ch in r1_qual) and \\\n",
    "               all(ord(ch) - 33 >= threshold for ch in r2_qual):\n",
    "                o1.write(\"\\n\".join(r1_lines) + \"\\n\")\n",
    "                o2.write(\"\\n\".join(r2_lines) + \"\\n\")\n",
    "\n",
    "# Function extract the section with the loop sequence\n",
    "def extract_sequence_between_markers(seq, start_marker, end_marker):\n",
    "    start_idx = seq.find(start_marker)\n",
    "    end_idx = seq.find(end_marker)\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:\n",
    "        return seq[start_idx + len(start_marker):end_idx]\n",
    "    return None\n",
    "\n",
    "# Function to see if for the same read R1 and R2 matches, if not, discard.\n",
    "# Also generate a count of how many reads are kept and are discarded.\n",
    "def filter_by_sequence_match(input_read1_file, input_read2_file, output_read1_file, output_read2_file):\n",
    "    with open(input_read1_file, 'r') as r1, open(input_read2_file, 'r') as r2, \\\n",
    "         open(output_read1_file, 'w') as o1, open(output_read2_file, 'w') as o2:\n",
    "\n",
    "         no_marker_r1_count = 0\n",
    "         no_marker_r2_count = 0\n",
    "         no_match_count = 0\n",
    "         total_count = 0\n",
    "\n",
    "         while True:\n",
    "            r1_lines = [r1.readline().strip() for _ in range(4)]\n",
    "            r2_lines = [r2.readline().strip() for _ in range(4)]\n",
    "\n",
    "            if not r1_lines[0] or not r2_lines[0]:\n",
    "                break\n",
    "\n",
    "            total_count += 1\n",
    "\n",
    "            sequence_r1 = extract_sequence_between_markers(r1_lines[1], start_marker_r1, end_marker_r1)\n",
    "            sequence_r2_extracted = extract_sequence_between_markers(r2_lines[1], start_marker_r2, end_marker_r2)\n",
    "            sequence_r2 = reverse_complement(sequence_r2_extracted) if sequence_r2_extracted else None\n",
    "            \n",
    "            if not sequence_r1:\n",
    "                no_marker_r1_count += 1\n",
    "            if not sequence_r2_extracted:\n",
    "                no_marker_r2_count += 1\n",
    "            if not sequence_r1 or not sequence_r2_extracted:\n",
    "                continue\n",
    "            \n",
    "            if sequence_r1 != sequence_r2:  \n",
    "                no_match_count += 1\n",
    "                continue\n",
    "\n",
    "            o1.write(\"\\n\".join(r1_lines) + \"\\n\")\n",
    "            o2.write(\"\\n\".join(r2_lines) + \"\\n\")\n",
    "\n",
    "         print(f\"Total reads: {total_count}\")\n",
    "         print(f\"Reads without markers in R1: {no_marker_r1_count}\")\n",
    "         print(f\"Reads without markers in R2: {no_marker_r2_count}\")\n",
    "         print(f\"Reads with markers but not matching: {no_match_count}\")\n",
    "\n",
    "# Combine everything needed for the function call.\n",
    "def process_files(directory_path):\n",
    "    files = sorted([f for f in os.listdir(directory_path) if f.endswith('.fastq')])\n",
    "\n",
    "    paired_files = {}\n",
    "    for file in files:\n",
    "        pieces = file.split('_')\n",
    "        prefix = pieces[1] + '_' + pieces[2] + '_' + pieces[3]\n",
    "        if \"R1\" in file:\n",
    "            paired_files[prefix] = paired_files.get(prefix, {})\n",
    "            paired_files[prefix]['R1'] = file\n",
    "        elif \"R2\" in file:\n",
    "            paired_files[prefix] = paired_files.get(prefix, {})\n",
    "            paired_files[prefix]['R2'] = file\n",
    "\n",
    "    for prefix, pair in paired_files.items():\n",
    "        if 'R1' in pair and 'R2' in pair:\n",
    "            read1_file_path = os.path.join(directory_path, pair['R1'])\n",
    "            read2_file_path = os.path.join(directory_path, pair['R2'])\n",
    "            \n",
    "            filtered_read1_file_path = os.path.join(directory_path, f\"filtered_{prefix}_R1.fastq\")\n",
    "            filtered_read2_file_path = os.path.join(directory_path, f\"filtered_{prefix}_R2.fastq\")\n",
    "            filter_reads_by_qscore(read1_file_path, read2_file_path, filtered_read1_file_path, filtered_read2_file_path)\n",
    "            \n",
    "            # Move the original files to Raw_Data\n",
    "            raw_data_dir = os.path.join(directory_path, 'Raw_Data')\n",
    "            if not os.path.exists(raw_data_dir):\n",
    "                os.makedirs(raw_data_dir)\n",
    "            shutil.move(read1_file_path, raw_data_dir)\n",
    "            shutil.move(read2_file_path, raw_data_dir)\n",
    "            print(\"Raw data move complete.\")\n",
    "\n",
    "            matched_read1_file_path = os.path.join(directory_path, f\"matched_{prefix}_R1.fastq\")\n",
    "            matched_read2_file_path = os.path.join(directory_path, f\"matched_{prefix}_R2.fastq\")\n",
    "            filter_by_sequence_match(filtered_read1_file_path, filtered_read2_file_path, matched_read1_file_path, matched_read2_file_path)\n",
    "            \n",
    "            # Move the filtered files to QualityFiltered_Data\n",
    "            quality_filtered_data_dir = os.path.join(directory_path, 'QualityFiltered_Data')\n",
    "            if not os.path.exists(quality_filtered_data_dir):\n",
    "                os.makedirs(quality_filtered_data_dir)\n",
    "            shutil.move(filtered_read1_file_path, quality_filtered_data_dir)\n",
    "            shutil.move(filtered_read2_file_path, quality_filtered_data_dir)\n",
    "            print(\"Filtered data move complete.\")\n",
    "\n",
    "            # Move the matched files to matched_R2\n",
    "            matched_R2_dir = os.path.join(directory_path, 'matched_R2')\n",
    "            if not os.path.exists(matched_R2_dir):\n",
    "                os.makedirs(matched_R2_dir)\n",
    "            shutil.move(matched_read2_file_path, matched_R2_dir)\n",
    "            print(\"Matched data move complete.\")\n",
    "\n",
    "\n",
    "# Call the function\n",
    "process_files(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating read counts for each loop based on ligand condition and bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes an empty DataFrame\n",
    "df_total = pd.DataFrame()\n",
    "\n",
    "# Open a text file in analysis_path to write the report\n",
    "report_filename = f\"ReportRawDataInSequence_Bins_{experimental_prefix}.txt\"\n",
    "with open(os.path.join(analysis_path, report_filename), 'w') as report_file:\n",
    "\n",
    "    # Here specify which replicate is being \n",
    "    for filename in glob.glob(os.path.join(data_path, 'matched*.fastq')):\n",
    "        count = 0\n",
    "        total_len = 0\n",
    "        seqs = []\n",
    "\n",
    "        with open(filename, 'r') as in_handle:\n",
    "            for title, seq, qual in FastqGeneralIterator(in_handle):\n",
    "                count += 1\n",
    "                total_len += len(seq)\n",
    "                seqs.append(seq)\n",
    "\n",
    "        info_string = \"%i records with total sequence length %i\" % (count, total_len)\n",
    "        print(info_string)\n",
    "        report_file.write(info_string + '\\n')\n",
    "\n",
    "        loops = []\n",
    "        correct = 0\n",
    "        fail = 0\n",
    "        total = 0\n",
    "        \n",
    "        for seq in seqs:\n",
    "            loop = ''\n",
    "            if 'CATATTATCTTATATGCCACAAAAAGCCGACCGTCTGGGC' in seq:\n",
    "                loop = seq.split('CATATTATCTTATATGCCACAAAAAGCCGACCGTCTGGGC')[1].split('GCCTGGATTGCGTCGGCTTTT')[0]\n",
    "                if min_loop_length <= len(loop) <= max_loop_length:  # Check if loop length is within the range\n",
    "                    if 'N' not in loop:\n",
    "                        loops.append(loop)\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        fail += 1\n",
    "                else:\n",
    "                    fail += 1\n",
    "                total += 1\n",
    "\n",
    "        counts = dict()\n",
    "        name = os.path.basename(filename).split('_')[3].split('S')[1]\n",
    "        col_name = int(name)\n",
    "\n",
    "        for i in sorted(loops):\n",
    "            counts[i] = counts.get(i, 0) + 1\n",
    "\n",
    "        df = pd.DataFrame.from_dict(counts, orient='index', dtype=None, columns=[col_name])\n",
    "\n",
    "        if df_total.empty:\n",
    "            df_total = df\n",
    "        else:\n",
    "            df_total = pd.concat([df_total, df], axis=1)\n",
    "\n",
    "        df_total = df_total.sort_index(axis=1)\n",
    "\n",
    "# Rename the first column as \"Loop\"\n",
    "df_total = df_total.rename_axis(\"Loop\", axis=1)\n",
    "\n",
    "# Saves df_total to a .csv file in analysis_path\n",
    "csv_filename = f\"RawData_Bins_{experimental_prefix}.csv\"\n",
    "df_total.to_csv(os.path.join(analysis_path, csv_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing read counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(analysis_path, f'RawData_Bins_{experimental_prefix}.csv')\n",
    "\n",
    "# Read in the original CSV file to a DataFrame\n",
    "df_original = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Read in the CSV file to a DataFrame for calculating the normalization ratios\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Exclude the first row and calculate the column-wise sum\n",
    "column_sums = df.iloc[1:].sum()\n",
    "\n",
    "# Create a new DataFrame to store the column sums\n",
    "df_sums = pd.DataFrame(column_sums, columns=['Sum'])\n",
    "\n",
    "# Rename the column\n",
    "df_sums = df_sums.rename(columns={'Sum': 'SUM of reads'})\n",
    "\n",
    "# Drop the first row\n",
    "df_sums = df_sums.drop(df_sums.index[0])\n",
    "\n",
    "# Read 'Cells counted' values from 'CellNumber.txt'\n",
    "with open(os.path.join(data_path, cell_counts), 'r') as f:\n",
    "    cells_counted_values = [int(line.strip()) for line in f if line.strip() and line.strip().isdigit()]\n",
    "\n",
    "# Add 'Cells counted' column and set the values for rows 2-21\n",
    "df_sums.loc[df_sums.index[1:21], 'Cells counted'] = cells_counted_values\n",
    "\n",
    "# Create 'Normalized reads' column\n",
    "max_sum_of_reads = df_sums['SUM of reads'].max()\n",
    "df_sums['Normalized reads'] = df_sums['SUM of reads'] / max_sum_of_reads\n",
    "\n",
    "# Create 'Normalized cells' column\n",
    "max_cells_counted = df_sums['Cells counted'].max()\n",
    "df_sums['Normalized cells'] = df_sums['Cells counted'] / max_cells_counted\n",
    "\n",
    "# Create 'Normalized Ratios' column\n",
    "df_sums['Normalized Ratios'] = df_sums['Normalized reads'] / df_sums['Normalized cells']\n",
    "\n",
    "print(df_sums)\n",
    "\n",
    "# Normalize the original DataFrame using the calculated 'Normalized Ratios'\n",
    "for column in df_original.columns[1:]:\n",
    "    df_original[column] = df_original[column] / df_sums.at[column, 'Normalized Ratios']\n",
    "\n",
    "# Construct the output filename using the experimental_suffix\n",
    "output_filename = os.path.join(analysis_path, f'NormalizedData_{experimental_prefix}.csv')\n",
    "\n",
    "# Write the normalized DataFrame to the new CSV file\n",
    "df_original.to_csv(output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing sparse data before calculating mean fluorescence estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file and the analysis path\n",
    "csv_file_path = os.path.join(analysis_path, f'NormalizedData_{experimental_prefix}.csv')\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define the column ranges to check\n",
    "column_ranges = [\n",
    "    (2, 5),\n",
    "    (6, 9),\n",
    "    (10, 13),\n",
    "    (14, 17),\n",
    "    (18, 21)\n",
    "]\n",
    "\n",
    "# Function to count non-zero and non-blank datapoints in a given range of columns\n",
    "def count_valid_data(df, start_col, end_col):\n",
    "    cols = [f'{i}' for i in range(start_col, end_col + 1)]\n",
    "    valid_data = df[cols].apply(pd.to_numeric, errors='coerce').fillna(0)  # Convert to numeric and treat errors as NaN, then fill NaN with 0\n",
    "    return (valid_data != 0).sum(axis=1)\n",
    "\n",
    "# Apply the function to each range and determine if the row meets the criteria for each range\n",
    "for start_col, end_col in column_ranges:\n",
    "    df[f'count_{start_col}_{end_col}'] = count_valid_data(df, start_col, end_col)\n",
    "\n",
    "# Define a condition for 'Good Data'\n",
    "# Check if each range count meets the criteria for each row\n",
    "conditions = [(df[f'count_{start_col}_{end_col}'] >= 3) for start_col, end_col in column_ranges]\n",
    "df['meets_criteria'] = pd.concat(conditions, axis=1).all(axis=1)\n",
    "\n",
    "# Split the data into 'GoodData' and 'BadData'\n",
    "good_data = df[df['meets_criteria']].fillna(0)  # Fill blank cells with 0\n",
    "bad_data = df[~df['meets_criteria']]\n",
    "\n",
    "# Output filenames\n",
    "good_data_output_filename = os.path.join(analysis_path, f'GoodDataforLogTransform_{experimental_prefix}.csv')\n",
    "bad_data_output_filename = os.path.join(analysis_path, f'BadDataforLogTransform_{experimental_prefix}.csv')\n",
    "\n",
    "# Save to CSV files\n",
    "good_data.to_csv(good_data_output_filename, index=False)\n",
    "bad_data.to_csv(bad_data_output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating mean fluorescence estimate for each loop in each ligand condition\n",
    "\n",
    "Average of read counts in each bin is weighted by the geometric mean fluorescence of each bin:\n",
    "\n",
    "Gate    | Range          | Geo Mean |\n",
    "--------|----------------|----------|\n",
    "Bin 1   | 2767 - 9114    | 5021     |\n",
    "Bin 2   | 9115 - 28952   | 16245    |\n",
    "Bin 3   | 28953 - 85559  | 49771    |\n",
    "Bin 4   | 85560 - 262143 | 149762   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(analysis_path, f'GoodDataforLogTransform_{experimental_prefix}.csv')\n",
    "\n",
    "# Read in the normalized data CSV file to a DataFrame\n",
    "df_normalized = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a new DataFrame\n",
    "df_0mM = pd.DataFrame()\n",
    "\n",
    "# Add the original first column to the new DataFrame\n",
    "df_0mM[df_normalized.columns[0]] = df_normalized.iloc[:, 0]\n",
    "\n",
    "# Apply the provided equation to the first column\n",
    "df_0mM['0mM'] = (np.log10(5021) * df_normalized.iloc[:, 2] +\n",
    "                 np.log10(16245) * df_normalized.iloc[:, 3] +\n",
    "                 np.log10(49771) * df_normalized.iloc[:, 4] +\n",
    "                 np.log10(149762) * df_normalized.iloc[:, 5]) / df_normalized.iloc[:, 2:6].sum(axis=1)\n",
    "\n",
    "# Apply the provided equation to each row for 0_01mM, 0_1mM, 0_32mM, and 1mM\n",
    "for i, concentration in zip(range(6, 22, 4), ['0_01mM', '0_1mM', '0_32mM', '1mM']):\n",
    "    df_0mM[concentration] = (np.log10(5021) * df_normalized.iloc[:, i] +\n",
    "                             np.log10(16245) * df_normalized.iloc[:, i+1] +\n",
    "                             np.log10(49771) * df_normalized.iloc[:, i+2] +\n",
    "                             np.log10(149762) * df_normalized.iloc[:, i+3]) / df_normalized.iloc[:, i:i+4].sum(axis=1)\n",
    "\n",
    "# Construct the output filename using the extracted name_part\n",
    "output_filename = os.path.join(analysis_path, f'LogTransformed_{experimental_prefix}.csv')\n",
    "\n",
    "# Write the DataFrame to the new CSV file\n",
    "df_0mM.to_csv(output_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your LogTransformed data CSV file\n",
    "csv_file_path = os.path.join(analysis_path, f'LogTransformed_{experimental_prefix}.csv')\n",
    "\n",
    "# Read in the LogTransformed.csv\n",
    "df_log_transformed = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Create a new DataFrame and copy the original first column\n",
    "df_multiplied = pd.DataFrame()\n",
    "df_multiplied[df_log_transformed.columns[0]] = df_log_transformed.iloc[:, 0]\n",
    "\n",
    "# Apply the transformation to the rest of the DataFrame\n",
    "df_multiplied[df_log_transformed.columns[1:]] = 10 ** df_log_transformed.iloc[:, 1:]\n",
    "\n",
    "# Construct the output filename using the experimental_suffix\n",
    "output_filename = os.path.join(analysis_path, f'MultipliedData_{experimental_prefix}.csv')\n",
    "\n",
    "# Write out to the new CSV file\n",
    "df_multiplied.to_csv(output_filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats_df = pd.read_csv(os.path.join(analysis_path, f'RawData_Bins_{experimental_prefix}.csv'))\n",
    "\n",
    "def count_rows_with_data(csv_file):\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Count the number of non-null values for each column\n",
    "        counts = df.count()\n",
    "\n",
    "        # Calculate the percentage of non-null values for each column relative to the first row\n",
    "        first_row_count = counts.iloc[0]\n",
    "        percentages = (counts / first_row_count) * 100\n",
    "\n",
    "        # Create a new DataFrame with the counts and column names\n",
    "        data_stats_df = pd.DataFrame({\"Sample Name\": counts.index, \n",
    "                                      \"Number of Loops recovered\": counts.values,\n",
    "                                      \"Percentage of loops recovered\": percentages.values})\n",
    "\n",
    "        # Print the statistics to the console\n",
    "        print(\"DataStats:\")\n",
    "        print(data_stats_df)\n",
    "\n",
    "        # Construct the output filename using the experimental_suffix\n",
    "        output_filename = os.path.join(command_path, f'DataStats_{experimental_prefix}.csv')\n",
    "\n",
    "        # Write the DataFrame to the new CSV file\n",
    "        data_stats_df.to_csv(output_filename, index=False)\n",
    "\n",
    "        print(f\"\\n{output_filename} file created successfully.\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "csv_file_path = os.path.join(analysis_path, f'RawData_Bins_{experimental_prefix}.csv')\n",
    "count_rows_with_data(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Part 1 of the code. Repeat this with data from the second replicate by changing `experimental_prefix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
